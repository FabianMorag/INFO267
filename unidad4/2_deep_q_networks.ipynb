{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Deep Reinforced Learning*\n",
    "\n",
    "Hoy en día el estado del arte en el reconocimiento de patrones está dominado por las **redes neuronales profundas**\n",
    "\n",
    "- Visión Computacional: Redes Neuronales Convolucionales, Adversarios generativos\n",
    "- Reconocimiento de habla: Redes Recurrentes, WaveNet\n",
    "- Procesamiento de lenguaje Natural: Transformers\n",
    "\n",
    "Las redes neuronales son excelentes para representar el mundo: modelos\n",
    "\n",
    "> Podemos aprovechar esta capacidad para diseñar mejores algoritmos de aprendizaje reforzado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuerdo: Value Learning\n",
    "\n",
    "En este tipo de algoritmos usamos un política de máxima utilidad para escoger acciones\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\text{arg} \\max_{a\\in \\mathcal{A}} Q(s, a)\n",
    "$$\n",
    "\n",
    "Y el problema entonces se reduce a aprender **Q**, *e.g* Q-Learning\n",
    "\n",
    "Sin embargo Q-learning tiene limitaciones: \n",
    "- requiere de heurísticas para explorar \n",
    "- espacio de acciones debe ser discreto\n",
    "- espacio de estados debe ser discreto\n",
    "\n",
    "De hecho el estado no puede ser demasiado grande, como veremos a continuación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambiente: [Space invaders](https://es.wikipedia.org/wiki/Space_Invaders)\n",
    "\n",
    "Originalmente un juego de Arcade lanzando en 1978, en 1980 tuvo una versión para ATARI 2600\n",
    "\n",
    "> El objetivo es derrivar a los extraterrestres usando un cañon antes de que desciendan a la Tierra\n",
    "\n",
    "- El cañon puede moverse a la izquierda, a la derecha y disparar\n",
    "- Hay cuatro escudos con los que el cañon puede protegerse de los disparos enemigos\n",
    "- Mientras menos enemigos más rápido se mueven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from time import sleep\n",
    "\n",
    "env = gym.make(\"SpaceInvaders-v0\") \n",
    "env.reset()\n",
    "end = False\n",
    "\n",
    "while not end:\n",
    "    a = env.action_space.sample()\n",
    "    s, r, end, info = env.step(a)\n",
    "    env.render() \n",
    "    sleep(.02)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espacio de estados\n",
    "display(env.observation_space)\n",
    "# Espacio de acciones\n",
    "display(env.action_space)\n",
    "display(env.action_space.n)\n",
    "display(env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo si usamos como estado un stack de 4 imágenes consecutivas y asumimos que cada pixel tiene 255 niveles:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "255**(210*160*3*4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aproximación de funciones\n",
    "\n",
    "Claramente, el espacio de estados del ejemplo anterior es imposible de mantener en una tabla Q\n",
    "\n",
    "Ojo: este espacio de estados está aun lejos de un problema del mundo real\n",
    "\n",
    "> Nos va a faltar memoría para guardar la tabla y datos para poder entrenar nuestro agente\n",
    "\n",
    "¿Qué podemos hacer?\n",
    "\n",
    "> Usar una representación más compacta para Q\n",
    "\n",
    "En lugar de tener una tabla con todos las combinaciones estado/acción podemos\n",
    "\n",
    "> Aproximar **Q** usando un **modelo paramétrico**\n",
    "\n",
    "Esta es la idea principal tras *Value function approximation* (VFA) y *Q function approximation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El caso más sencillo es usar un **modelo lineal en sus parámetros**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat Q_\\theta(s,a) &= \\theta_0 \\phi_0(s, a) + \\theta_1 \\phi_1(s, a) + \\theta_2 \\phi_2(s, a) + \\ldots + \\theta_M \\phi_M (s,a) \\nonumber \\\\\n",
    "&= \\sum_{j=0}^M \\theta_j \\phi_j (s,a) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde \n",
    "- $\\{ \\theta\\}$ es un vector de parámetros con $M+1$ componentes\n",
    "- $\\{\\phi\\}$ es un conjunto de funciones base, *e.g.* polinomios, Fourier, árbol de decisión, kernels\n",
    "\n",
    "En lugar de aprender $Q$ explicitamente el objetivo es aprender $\\theta$\n",
    "\n",
    "> La cantidad de parámetros es ahora independiente de la dimensionalidad del estado\n",
    "\n",
    "\n",
    "\n",
    "Al igual que antes nuestro objetivo es acercanos a la solución de la Ecuación de Bellman\n",
    "\n",
    "Podemos escribir esto como el siguiente problema de optimización\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\| R(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}} \\hat Q_\\theta( s',a') - \\hat Q_\\theta(s,a)\\|^2\n",
    "$$\n",
    "\n",
    "de donde podemos aprender $\\theta$ iterativamente usando usando gradiente descendente \n",
    "\n",
    "$$\n",
    "\\theta_j \\leftarrow \\theta_j + 2 \\alpha \\left(R(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}} \\hat Q_\\theta (s',a') - \\hat Q_\\theta(s,a) \\right) \\phi_j(s,a)\n",
    "$$\n",
    "\n",
    "Sin embargo, un modelo lineal podría ser muy limitado\n",
    "\n",
    "En la unidad 1 estudiamos el estado del arte en aproximación de funciones: **redes neuronales artificiales**\n",
    "\n",
    "> A continuación veremos como usar redes profundas para aproximar la función Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN) \n",
    "\n",
    "> En [(Minh et al. 2013)](https://arxiv.org/abs/1312.5602) se usaron redes neuronales profundas de tipo convolucional para resolver una serie de juegos de ATARI con Aprendizaje Reforzardo obteniendo [desempeño sobre-humano en muchos de las pruebas](https://deepmind.com/blog/article/deep-reinforcement-learning). El modelo, llamado *Deep Q-network*, utiliza como estado el valor de todos los píxeles de cuatro cuadros consecutivos.\n",
    "\n",
    "La idea clave es\n",
    "\n",
    "> Aprovechar la capacidad de las redes neuronales profundas para representar datos complejos, e.g. imágenes \n",
    "\n",
    "o más en concreto\n",
    "\n",
    "> Aproximar la función Q usando una red convolucional entrenada directamente sobre los píxeles\n",
    "\n",
    "Veremos primero una formulación general y luego su aplicación al caso de imágenes\n",
    "\n",
    "### Modelo DQN\n",
    "\n",
    "- La entrada de la red es el estado $s$. \n",
    "    - El vector de estado puede tener valores continuos o discretos\n",
    "- La salida de la red neuronal son los valores $Q(s, a_1), Q(s, a_2), \\ldots, Q(s, a_N)$\n",
    "    - Se considera un espacio de acciones discreto\n",
    "    - Esto es más eficiente que considerar $a'$ como una entrada y retornar $Q(s,a')$\n",
    "- La cantidad y tipo de las capas intermedias es decisión del usuario\n",
    "    - Si tenemos datos continuos (atributos) usamos capas completamente conectadas\n",
    "    - Si usamos píxeles es natural usar capas convolucionales\n",
    "- La función de perdida que se ocupa en DQN es el error cuadrático medio entre la ecuación de Bellman y la predicción de la red\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\mathbb{E}\\left[\\left \\| R(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}} Q_\\theta(s', a') - Q_\\theta(s, a)\\right \\|^2\\right]\n",
    "$$\n",
    "\n",
    "Recordemos: $s'$ es el estado al que llegamos luego de ejecutar $a$ sobre $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN con estados continuos: El retorno del carro con péndulo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previamente fue necesario discretizar el estado para construir la matriz Q\n",
    "\n",
    "En DQN podemos obviar este paso y sus complicaciones\n",
    "\n",
    "Vamos a usar una red neuronal con 3 capas completamente conectadas, 4 entradas y 2 salidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class DQN_FC(torch.nn.Module):    \n",
    "    def __init__(self, n_input, n_output, n_hidden=10):\n",
    "        super(DQN_FC, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(n_input, n_hidden)\n",
    "        self.linear2 = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear3 = torch.nn.Linear(n_hidden, n_output)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.activation(self.linear1(x))\n",
    "        h = self.activation(self.linear2(h))\n",
    "        return  self.linear3(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(3, figsize=(6, 5), sharex=True, tight_layout=True)\n",
    "\n",
    "def update_plot():\n",
    "    for ax_ in ax:\n",
    "        ax_.cla()\n",
    "    \n",
    "    episodes = np.arange((episode+1)//100)*100\n",
    "    ax[0].errorbar(episodes,\n",
    "                   np.array(diagnostics['rewards']).mean(axis=1), \n",
    "                   np.array(diagnostics['rewards']).std(axis=1));\n",
    "    ax[0].set_ylabel('Recompensa\\npromedio');\n",
    "    ax[1].errorbar(episodes,\n",
    "                   np.array(diagnostics['episode_length']).mean(axis=1), \n",
    "                   np.array(diagnostics['episode_length']).std(axis=1));\n",
    "    ax[1].plot(episodes, [195]*len(episodes), 'k--')\n",
    "    ax[1].set_ylabel('Largo promedio\\nde los episodios');\n",
    "    ax[2].plot(episodes, epsilon(episodes))\n",
    "    ax[2].set_ylabel('Epsilon')\n",
    "    ax[2].set_xlabel('Episodios')\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consideraciones\n",
    "\n",
    "- El optimizador a usar es ADAM: Gradiente descendente con tasa de aprendizaje adaptiva\n",
    "- Se usa la heurística $\\epsilon$ greedy para favorecer la exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "model_policy = DQN_pole(n_hidden=10) \n",
    "double_dqn = False\n",
    "if double_dqn:\n",
    "    model_target = DQN_pole(n_hidden=10)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# criterion = torch.nn.SmoothL1Loss() # Huber Loss\n",
    "optimizer = torch.optim.Adam(model_policy.parameters(), lr=1e-3)\n",
    "\n",
    "diagnostics = {'rewards': [], 'episode_length': []}\n",
    "# Parametros\n",
    "gamma = 0.999\n",
    "epsilon_init = 1.0 \n",
    "epsilon_end = 0.01 \n",
    "epsilon_rate = 1e-3\n",
    "epsilon = lambda episode : epsilon_end + (epsilon_init - epsilon_end) * np.exp(-epsilon_rate*episode) \n",
    "\n",
    "for episode in tqdm(range(3000)):\n",
    "    env.reset()\n",
    "    end = False\n",
    "    # Entrenamiento\n",
    "    while not end:        \n",
    "        # Seleccionar la acción        \n",
    "        s_current = torch.from_numpy(np.array(env.state).astype('float32')) # NUEVO\n",
    "        pred = model_policy(s_current) # NUEVO\n",
    "        if not np.random.binomial(1, p=1.-epsilon(episode)): \n",
    "            Q_present = pred.max() # NUEVO\n",
    "            a = pred.argmax().item() # NUEVO  \n",
    "        else:                        \n",
    "            a = env.action_space.sample() \n",
    "            Q_present = pred[a]\n",
    "        # Ejecutar la acción\n",
    "        s, r, end, info = env.step(a)\n",
    "        # Actualizar la red Q\n",
    "        s_future = torch.from_numpy(np.array(s).astype('float32')) # NUEVO\n",
    "        \n",
    "        if double_dqn:\n",
    "            Q_future = model_target.predict(s_future)\n",
    "        else:\n",
    "            Q_future = model_policy(s_future)\n",
    "        \n",
    "        target = torch.tensor(r)\n",
    "        if not end:\n",
    "            target += gamma*Q_future.max().detach()\n",
    "        \n",
    "        loss = criterion(target, Q_present)\n",
    "        optimizer.zero_grad() # NUEVO\n",
    "        loss.backward() # NUEVO\n",
    "        optimizer.step() # NUEVO\n",
    "                \n",
    "        if double_dqn:\n",
    "            if episode % 10 == 0:\n",
    "                model_target.load_state_dict(model_policy.state_dict())\n",
    "\n",
    "    # Prueba\n",
    "    # Cada 100 epocas evaluamos nuestro agente\n",
    "    if np.mod(episode+1, 100) == 0:\n",
    "        diagnostics['rewards'].append(np.zeros(shape=(10,)))\n",
    "        diagnostics['episode_length'].append(np.zeros(shape=(10,)))\n",
    "        for k in range(10):\n",
    "            env.reset()    \n",
    "            end = False\n",
    "            episode_length = 0\n",
    "            episode_reward = 0.0\n",
    "            while not end:        \n",
    "                s_current = torch.from_numpy(np.array(env.state).astype('float32')) \n",
    "                a = model_policy(s_current).argmax().detach().numpy() \n",
    "                s_future, r, end, info = env.step(a)\n",
    "                episode_length += 1\n",
    "                episode_reward += r\n",
    "            \n",
    "            diagnostics['rewards'][-1][k] = episode_reward\n",
    "            diagnostics['episode_length'][-1][k] = episode_length\n",
    "        update_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, nuestro agente en acción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "end = False\n",
    "\n",
    "for k in range(500):\n",
    "    env.render()\n",
    "    s_current = torch.from_numpy(np.array(env.state).astype('float32'))\n",
    "    a = model_policy(s_current).argmax().detach().numpy() \n",
    "    s_future, r, end, info = env.step(a)\n",
    "    #if end:\n",
    "    #    break\n",
    "    if r == 0:\n",
    "        display(k)\n",
    "        break\n",
    "display(end, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay y Memory replay\n",
    "\n",
    "El entrenamiento usando sólo una instancia es bastante ruidoso (y un poco lento)\n",
    "\n",
    "Consideremos también que estamos entrenando con muestras muy correlacionadas (no iid)\n",
    "\n",
    "Sabemos que esto puede introducir sesgos el entrenamiento de una red neuronal\n",
    "\n",
    "> Para entrenar la DQN adecuadamente  (Minh et al 2013) propone un astuto \"truco\" llamado *Experience replay*  \n",
    "\n",
    "Esto consiste en almacenar la historia del agente en una memoria: *replay memory*\n",
    "\n",
    "Cada elemento en la memoria es una tupla $(s_t, a_t, r_{t+1}, s_{t+1})$\n",
    "\n",
    "Con esto se crean mini-batches en orden aleatorio para entrenar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, state_dim, memory_length=1024):\n",
    "        self.length = memory_length\n",
    "        self.pointer = 0\n",
    "        self.filled = False\n",
    "        # Tensores vacíos para la historia\n",
    "        self.s_current = torch.zeros(memory_length, state_dim)\n",
    "        self.s_future = torch.zeros(memory_length, state_dim)\n",
    "        self.a = torch.zeros(memory_length, 1, dtype=int)\n",
    "        self.r = torch.zeros(memory_length, 1)\n",
    "        # Adicionalmente guardaremos la condición de término\n",
    "        self.end = torch.zeros(memory_length, 1, dtype=bool)\n",
    "    \n",
    "    def push(self, s_current, s_future, a, r, end):\n",
    "        # Agregamos una tupla en la memoria\n",
    "        self.s_current[self.pointer] = s_current\n",
    "        self.s_future[self.pointer] = s_future\n",
    "        self.a[self.pointer] = a\n",
    "        self.r[self.pointer] = r \n",
    "        self.end[self.pointer] = end\n",
    "        if self.pointer + 1 == self.length:\n",
    "            self.filled = True\n",
    "        self.pointer =  (self.pointer + 1) % self.length    \n",
    "        \n",
    "    def sample(self, size=128):\n",
    "        # Extraemos una muestra aleatoria de la memoria\n",
    "        idx = np.random.choice(self.length, size)\n",
    "        return self.s_current[idx], self.s_future[idx], self.a[idx], self.r[idx], self.end[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Double DQN*\n",
    "\n",
    "\n",
    "En [(van Hasselt, Guer y Silver, 2015)](https://arxiv.org/pdf/1509.06461.pdf) los autores notaron un problema importante en DQN. \n",
    "\n",
    "> Cuando calculamos $Q(s, a)$ y $\\max Q(s', a')$ usando la misma red es muy posible que sobre-estimemos la calidad de nuestro objetivo. Adicionalmente si el objetivo cambia constantemente el entrenamiento será inestable\n",
    "\n",
    "\n",
    "La solución propuesta consiste en usar redes neuronales distintas para la escoger la acción y para calcular el objetivo (ecuación de Bellman)\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\phi) = \\mathbb{E}\\left[\\left \\| R(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}} Q_\\phi(s', a') - Q_\\theta(s, a)\\right\\|^2\\right]\n",
    "$$\n",
    "\n",
    "Usamos $Q_\\theta$ con parámetros $\\theta$ para escoger la acción: *policy network*\n",
    "\n",
    "Usamos $Q_\\phi$ con parámetros $\\phi$ para construir el objetivo: *target network\n",
    "\n",
    "- Ambas redes comparten arquitectura y cantidad de neuronas\n",
    "- Sólo se optimiza la *policy network*\n",
    "- Después de un cierto número de épocas los parametros de la policy network \"se copian\" en la *target network*\n",
    "\n",
    "¿Cada cuantas épocas actualizo la *target network*? Otro hyperparámetro para el algoritmo... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve\n",
    "\n",
    "fig, ax = plt.subplots(3, figsize=(6, 5), sharex=True, tight_layout=True)\n",
    "\n",
    "def update_plot(episode, smooth_window=10):\n",
    "    for ax_ in ax:\n",
    "        ax_.cla()\n",
    "    episodes = np.arange((episode))\n",
    "    if episode > smooth_window:\n",
    "        ax[0].plot(episodes[:-smooth_window+1], \n",
    "                   convolve(diagnostics['rewards'][:episode], \n",
    "                            np.ones(smooth_window), mode='valid')/smooth_window)        \n",
    "        ax[1].plot(episodes[:-smooth_window+1], \n",
    "                   convolve(diagnostics['loss'][:episode], \n",
    "                            np.ones(smooth_window), mode='valid')/smooth_window)\n",
    "    ax[0].plot(episodes, [195]*len(episodes), 'k--')\n",
    "    ax[0].set_ylabel('Recompensa\\npromedio');\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[2].plot(episodes, epsilon(episodes))\n",
    "    ax[2].set_ylabel('Epsilon')\n",
    "    ax[2].set_xlabel('Episodios')\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "n_state = env.observation_space.shape[0] # Número de estados\n",
    "n_action = env.action_space.n # Número de acciones\n",
    "\n",
    "model_policy = DQN_FC(n_state, n_action, n_hidden=100) \n",
    "\n",
    "double_dqn = False\n",
    "if double_dqn:\n",
    "    model_target = DQN_FC(n_state, n_action, n_hidden=100)\n",
    "    \n",
    "criterion = torch.nn.MSELoss() # Error medio cuadrático\n",
    "#criterion = torch.nn.SmoothL1Loss() # Huber Loss\n",
    "# Solo optimizaremos la policy network\n",
    "optimizer = torch.optim.Adam(model_policy.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "gamma = 0.999\n",
    "def epsilon(episode, epsilon_init=1.0, epsilon_end=0.01, epsilon_rate=1e-3):\n",
    "    return epsilon_end + (epsilon_init - epsilon_end) * np.exp(-epsilon_rate*episode) \n",
    "\n",
    "memory = ReplayMemory(n_state)        \n",
    "num_episodes = 3000\n",
    "diagnostics = {'rewards': np.zeros(shape=(3000,)), \n",
    "               'loss': np.zeros(shape=(3000,))}\n",
    "\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    env.reset()\n",
    "    end = False\n",
    "    episode_reward = 0\n",
    "    episode_loss = 0.0\n",
    "    while not end:        \n",
    "        # Llenar memoria\n",
    "        s_current = torch.tensor(env.state).float()\n",
    "        if not np.random.binomial(1, p=1.-epsilon(episode)): \n",
    "            with torch.no_grad():\n",
    "                a = model_policy(s_current).argmax().item() # NUEVO  \n",
    "        else:                        \n",
    "            a = env.action_space.sample() \n",
    "        s, r, end, info = env.step(a)\n",
    "        episode_reward += r\n",
    "        memory.push(s_current, torch.tensor(s).float(), a, r, end)\n",
    "        \n",
    "        # Entrenamiento\n",
    "        if memory.filled:\n",
    "            state_torch, state_future_torch, action_torch, reward_torch, end_torch = memory.sample()\n",
    "            # Obtener el valor de Q del estado actual y de la acción actual\n",
    "            prediction = model_policy(state_torch).gather(1, action_torch)\n",
    "            # Obtener el valor del mejor Q del estado futuro\n",
    "            if not double_dqn:\n",
    "                Q_future_best = model_policy(state_future_torch).max(1, keepdim=True)[0].detach()\n",
    "            else:\n",
    "                Q_future_best = model_target(state_future_torch).max(1, keepdim=True)[0].detach()\n",
    "            # Construir el target: r + gamma*max Q(s', a')\n",
    "            target = reward_torch\n",
    "            target[~end_torch] += gamma*Q_future_best[~end_torch]\n",
    "            # Actualizar\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(prediction, target)\n",
    "            loss.backward()\n",
    "            #for param in model_policy.parameters():\n",
    "            #    param.grad.data.clamp_(-1, 1)\n",
    "            optimizer.step()\n",
    "            episode_loss += loss.item()\n",
    "    \n",
    "    diagnostics['rewards'][episode] = episode_reward \n",
    "    diagnostics['loss'][episode] = episode_loss \n",
    "    \n",
    "    if double_dqn:\n",
    "        if episode % 10 == 0:\n",
    "            model_target.load_state_dict(model_policy.state_dict())\n",
    "                \n",
    "    if episode % 50 == 0:\n",
    "        update_plot(episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
