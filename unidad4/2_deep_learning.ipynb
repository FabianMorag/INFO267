{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://arxiv.org/abs/1811.12560\n",
    "- https://pathmind.com/wiki/deep-reinforcement-learning    \n",
    "- https://towardsdatascience.com/why-going-from-implementing-q-learning-to-deep-q-learning-can-be-difficult-36e7ea1648af   \n",
    "https://towardsdatascience.com/policy-based-reinforcement-learning-the-easy-way-8de9a3356083"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/deepmind/pysc2\n",
    "- http://chris-chris.ai/2017/08/30/pysc2-tutorial1/\n",
    "- https://github.com/chris-chris/pysc2-examples\n",
    "- https://blog.goodaudience.com/lessons-and-mistakes-from-my-first-reinforcement-learning-starcraft-agent-4245cc35e956\n",
    "- https://faculty.utrgv.edu/dongchul.kim/csci4352/spring2019/report/R5.pdf\n",
    "- http://courses.cecs.anu.edu.au/courses/CSPROJECTS/19S1/reports/u6049249_report.pdf\n",
    "- https://chatbotslife.com/building-a-smart-pysc2-agent-cdc269cb095d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://cs.stanford.edu/people/karpathy/reinforcejs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Reinforced Learning\n",
    "\n",
    "Deep Neural Networks's power to represent the world\n",
    "\n",
    "The ability to act on that understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning: Takes samples from data, encode the representation in a way we can reason about it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential decision making problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An inteligenet agent has to make decisions that affect the world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we learn anything? Starting from so little?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the frameworks of DL. The neural net is doign the representation of the world on which the actions are made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value learning\n",
    "\n",
    "Find $Q(s,t)$, then $a = argmax_a Q(s,t)$\n",
    "\n",
    "Deep Q network (DQN)\n",
    "\n",
    "Neural network that recieves a state and returns $Q(s, a_1), Q(s, a_2), \\ldots, Q(s, a_N)$\n",
    "\n",
    "maximize target return\n",
    "\n",
    "target is $r$ current reward plus \\gamma \\max_a' Q(a',s')$\n",
    "\n",
    "prediced is $Q(s, a)$\n",
    "\n",
    "loss is $\\mathbb{E}[\\| (r + \\gamma \\max_a' Q(a',s') - Q(s,a)\\|^2]$ called Q-loss\n",
    "\n",
    "Q-learning, discrete and small action space, cannot handle continous action space, does not learn stochastic policies, we solve some of this with policy gradient learning\n",
    "\n",
    "Policy learning\n",
    "\n",
    "Find $\\pi(s)$, sample $a\\sim\\pi(s)$\n",
    "\n",
    "Neural network that recieves a state and returns $P(a_1|s), P(a_2|s), \\ldots, P(a_N|s)$. You draw a sample from this distribution\n",
    "\n",
    "advantages? distribution does not need to be categorical, can handle continuous, parametrize $\\mu$ and $\\sigma$ \n",
    "\n",
    "\n",
    "Training algorithm\n",
    "\n",
    "- Run policy until termination, record tuples actions/states/rewards\n",
    "- Decrease pbb of actions that ended on low reward \n",
    "- Increase pbb of actions that ended on high reward\n",
    "\n",
    "$$\n",
    "L = - \\log P(a_t|s_t) R_t\n",
    "$$\n",
    "\n",
    "log likelihood of selecting the action, how likely was this action that you selected\n",
    "\n",
    "total discounted returned recieved by selecting that action\n",
    "\n",
    "- high reward times likely action -> desirable, will not change\n",
    "- reward is low times likely action -> undesirable, will minimize: remove pbb of this action\n",
    "\n",
    "$$\n",
    "w = w + \\mu \\nabla \\log P(a_t|s_t) R_t\n",
    "$$\n",
    "\n",
    "policy gradient\n",
    "\n",
    "real life: run until termination!!!!! Requires simulator!!!!\n",
    "- realistic simulation + transfer learning\n",
    "- real world observations + one shot trial & error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agente\n",
    "\n",
    "El agente está basado en uno o más de los siguientes componentes\n",
    "\n",
    "- Policy: Indica como escogemos las acciones, comportamiento del agente\n",
    "- Value: Se usa para evaluar la \"calidad\" de los estados estados\n",
    "- Model: El agente mantiene una representación aproximada del ambiente \n",
    "\n",
    "\n",
    "No siempre están presentes las tres\n",
    "\n",
    "- Agentes basados en modelo versus agentes *model-free*\n",
    "- Agentes basados en policy versus agentes basados en value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Modelo/*Model*\n",
    "\n",
    "Hay dos cosas que el agente busca modelar\n",
    "\n",
    "¿Cúal será el siguiente estado?\n",
    "\n",
    "Se modela como una probabilidad de transicionar al estado $s'$ dado que estamos en $s$ y tomamos la acción $a$\n",
    "\n",
    "$$\n",
    "\\mathcal{P}_{ss'}^a = P(S_{t+1}=s'|S_t=s|A_t =a)\n",
    "$$\n",
    "\n",
    "\n",
    "¿Cúal será la siguiente recompensa?\n",
    "\n",
    "Se modela como el valor esperado de recompensa dado que tomo la acción $a$ en el estado $s$\n",
    "\n",
    "$$\n",
    "\\mathcal{R}_s^a = \\mathbb{E}[R|S_t=s, A_t=a]\n",
    "$$\n",
    "\n",
    "Expected total future reward Q function\n",
    "\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) = \\mathbb{E}[R_t|s_t, a_t]\n",
    "$$\n",
    "\n",
    "The policy should choose an action that maximizes future reward\n",
    "$$\n",
    "\\pi(s)^* = argmax_a Q(s_t, a_t)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@curiousily/solving-an-mdp-with-q-learning-from-scratch-deep-reinforcement-learning-for-hackers-part-1-45d1d360c120\n",
    "\n",
    "https://github.com/aamini/introtodeeplearning/blob/master/lab3/RL.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
