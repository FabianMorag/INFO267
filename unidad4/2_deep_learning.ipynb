{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Deep Reinforced Learning*\n",
    "\n",
    "Hoy en día el estado del arte en el reconocimiento de patrones está dominado por las **redes neuronales profundas**\n",
    "\n",
    "- Visión Computacional: Redes Neuronales Convolucionales, Adversarios generativos\n",
    "- Reconocimiento de habla: Redes Recurrentes, WaveNet\n",
    "- Procesamiento de lenguaje Natural: Transformers\n",
    "\n",
    "> Podemos aprovechar la habilidad de las redes profundas para representar el mundo para diseñar mejores algoritmos de aprendizaje reforzado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En base a redes profundas se han obtenido algoritmos de aprendizaje reforzado con desempeño muy superior a Q-Learning: **Deep RL**\n",
    "\n",
    "> Aprovechar la habilidad de las redes neuronales de representar el mundo: modelos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Learning\n",
    "\n",
    "En este tipo de algoritmos usamos un política de máxima utilidad para escoger acciones\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\text{arg} \\max_{a\\in \\mathcal{A}} Q(s, a)\n",
    "$$\n",
    "\n",
    "Y el problema entonces se reduce a aprender **Q**, *e.g* Q-Learning\n",
    "\n",
    "Sin embargo Q-learning tiene limitaciones: \n",
    "- requiere de heurísticas para explorar \n",
    "- espacio de acciones debe ser discreto\n",
    "- espacio de estados debe ser discreto\n",
    "\n",
    "De hecho el estado no puede ser demasiado grande, como veremos a continuación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambiente: [Space invaders](https://es.wikipedia.org/wiki/Space_Invaders)\n",
    "\n",
    "Originalmente un juego de Arcade lanzando en 1978, en 1980 tuvo una versión para ATARI 2600\n",
    "\n",
    "> El objetivo es derrivar a los extraterrestres usando un cañon antes de que desciendan a la Tierra\n",
    "\n",
    "- El cañon puede moverse a la izquierda, a la derecha y disparar\n",
    "- Hay cuatro escudos con los que el cañon puede protegerse de los disparos enemigos\n",
    "- Mientras menos enemigos más rápido se mueven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from time import sleep\n",
    "\n",
    "env = gym.make(\"SpaceInvaders-v0\") \n",
    "env.reset()\n",
    "end = False\n",
    "\n",
    "while not end:\n",
    "    a = env.action_space.sample()\n",
    "    s, r, end, info = env.step(a)\n",
    "    env.render() \n",
    "    sleep(.02)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espacio de estados\n",
    "display(env.observation_space)\n",
    "# Espacio de acciones\n",
    "display(env.action_space)\n",
    "display(env.action_space.n)\n",
    "display(env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo si usaramos un stack de 4 imágenes consecutivas como estado y asumiendo que cada pixel tiene 255 niveles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "255**(210*160*4*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claramente, llenar una tabla Q con esta cantidad de estados no es práctico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN) \n",
    "\n",
    "Propuesto en [(Minh et al. 2013)](https://arxiv.org/abs/1312.5602)\n",
    "\n",
    "#### Idea clave\n",
    "\n",
    "> Se puede usar una red convolucional como representación del estado (mundo)\n",
    "\n",
    "- La entrada de la red neuronal es el estado $s$, el estado puede ser continuo o discreto\n",
    "- La salida de la red neuronal son los valores $Q(s, a_1), Q(s, a_2), \\ldots, Q(s, a_N)$\n",
    "\n",
    "> Ya no es necesario mantener una tabla Q que crece con el número de estados\n",
    "\n",
    "Sin embargo aun requerimos que le espacio de acciones sea discreto\n",
    "\n",
    "\n",
    "Esto se conoce como **Deep Q-Network** y es un tipo de value-learning\n",
    "\n",
    "(Previamente se usaba una red que recibe $s$ y $a$ y retorna $Q(s,a)$, ¿por qué lo expuesto arriba es superior?)\n",
    "\n",
    "\n",
    "#### Función de pérdida (loss)\n",
    "\n",
    "La función de perdida que se ocupa en DQN es el error cuadrático medio entre la ecuación de Bellman y la predicción de la red\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\mathbb{E}\\left[\\left \\| R(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}} Q_\\theta(s', a') - Q_\\theta(s, a)\\right \\|^2\\right]\n",
    "$$\n",
    "\n",
    "Recordemos: $s'$ es el estado al que llegamos luego de ejecutar $a$ sobre $s$\n",
    "\n",
    "**Double DQN:** Usar redes neuronales distintas para la predicción y el objetivo tiende a reducir el sesgo en que incurre la red en las primeras iteraciones\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\phi) = \\mathbb{E}\\left[\\left \\| R(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}} Q_\\phi(s', a') - Q_\\theta(s, a)\\right\\|^2\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trucos\n",
    "\n",
    "Para mejorar la estababilidad del entrenamiento se propone lo siguiente:\n",
    "\n",
    "#### Experience Replay\n",
    "\n",
    "Consiste en almacenar conjuntos de estados, acciones y recompensas consecutivas\n",
    "\n",
    "Luego se crean mini-batches a partir de ellos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DQN alcanza destreza sobre-humana en 50 juegos de ATARI sin conocimiento a priori](https://deepmind.com/blog/article/deep-reinforcement-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN para el carro con péndulo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previamente fue necesario discretizar el estado para construir la matriz Q\n",
    "\n",
    "En DQN podemos obviar este paso y sus complicaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_pole(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=10):\n",
    "        super(DQN_pole, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(4, n_hidden)\n",
    "        self.linear2 = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear3 = torch.nn.Linear(n_hidden, 2)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.activation(self.linear1(x))\n",
    "        h = self.activation(self.linear2(x))\n",
    "        h = self.linear3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(3, figsize=(6, 5), sharex=True, tight_layout=True)\n",
    "\n",
    "def update_plot():\n",
    "    for ax_ in ax:\n",
    "        ax_.cla()\n",
    "    \n",
    "    episodes = np.arange((episode+1)//100)*100\n",
    "    ax[0].errorbar(episodes,\n",
    "                   np.array(diagnostics['rewards']).mean(axis=1), \n",
    "                   np.array(diagnostics['rewards']).std(axis=1));\n",
    "    ax[0].set_ylabel('Recompensa\\npromedio');\n",
    "    ax[1].errorbar(episodes,\n",
    "                   np.array(diagnostics['episode_length']).mean(axis=1), \n",
    "                   np.array(diagnostics['episode_length']).std(axis=1));\n",
    "    ax[1].plot(episodes, [195]*len(episodes), 'k--')\n",
    "    ax[1].set_ylabel('Largo promedio\\nde los episodios');\n",
    "    ax[2].plot(episodes, epsilon(episodes))\n",
    "    ax[2].set_ylabel('Epsilon')\n",
    "    ax[2].set_xlabel('Episodios')\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "model_Q = DQN_pole() # NUEVO\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "diagnostics = {'rewards': [], 'episode_length': []}\n",
    "# Parametros\n",
    "alpha = lambda episode: 0.1\n",
    "#alpha = lambda episode: 0.01 + (1. - 0.01) * np.exp(-epsilon_rate*episode) \n",
    "gamma = 0.999\n",
    "epsilon_init = 1.0 \n",
    "epsilon_end = 0.01 \n",
    "epsilon_rate = 1e-3\n",
    "epsilon = lambda episode : epsilon_end + (epsilon_init - epsilon_end) * np.exp(-epsilon_rate*episode) \n",
    "for episode in tqdm(range(3000)):\n",
    "    env.reset()\n",
    "    end = False\n",
    "    # Entrenamiento\n",
    "    while not end:        \n",
    "        s_current = torch.from_numpy(np.array(env.state).astype('float32')) # NUEVO\n",
    "        # Seleccionar la acción\n",
    "        if not np.random.binomial(1, p=1.-epsilon(episode)): \n",
    "            Q_present = model_Q(s_current)\n",
    "            a = Q_present.argmax(1).detach().numpy()            \n",
    "        else:\n",
    "            a = env.action_space.sample() \n",
    "        # Ejecutarla\n",
    "        s, r, end, info = env.step(a)\n",
    "        s_future = get_state(s) # NUEVO\n",
    "        # Actualizar Q\n",
    "        #Q[s_current][a] += alpha(episode)*(r + gamma*np.max(Q[s_future]) - Q[s_current][a]) \n",
    "        loss = 1\n",
    "        loss.backward()\n",
    "        optimizar.step()\n",
    "\n",
    "    # Prueba\n",
    "    # Cada 100 epocas evaluamos nuestro agente\n",
    "    if np.mod(episode+1, 100) == 0:\n",
    "        diagnostics['rewards'].append(np.zeros(shape=(10,)))\n",
    "        diagnostics['episode_length'].append(np.zeros(shape=(10,)))\n",
    "        for k in range(10):\n",
    "            env.reset()    \n",
    "            end = False\n",
    "            episode_length = 0\n",
    "            episode_reward = 0.0\n",
    "            while not end:        \n",
    "                s_current = get_state(env.state)\n",
    "                a = np.argmax(Q[s_current])  \n",
    "                s_future, r, end, info = env.step(a)\n",
    "                episode_length += 1\n",
    "                episode_reward += r\n",
    "            \n",
    "            diagnostics['rewards'][-1][k] = episode_reward\n",
    "            diagnostics['episode_length'][-1][k] = episode_length\n",
    "        update_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy learning\n",
    "\n",
    "Find $\\pi(s)$, sample $a\\sim\\pi(s)$\n",
    "\n",
    "Neural network that recieves a state and returns $P(a_1|s), P(a_2|s), \\ldots, P(a_N|s)$. You draw a sample from this distribution\n",
    "\n",
    "advantages? distribution does not need to be categorical, can handle continuous, parametrize $\\mu$ and $\\sigma$ \n",
    "\n",
    "\n",
    "Training algorithm\n",
    "\n",
    "- Run policy until termination, record tuples actions/states/rewards\n",
    "- Decrease pbb of actions that ended on low reward \n",
    "- Increase pbb of actions that ended on high reward\n",
    "\n",
    "$$\n",
    "L = - \\log P(a_t|s_t) R_t\n",
    "$$\n",
    "\n",
    "log likelihood of selecting the action, how likely was this action that you selected\n",
    "\n",
    "total discounted returned recieved by selecting that action\n",
    "\n",
    "- high reward times likely action -> desirable, will not change\n",
    "- reward is low times likely action -> undesirable, will minimize: remove pbb of this action\n",
    "\n",
    "$$\n",
    "w = w + \\mu \\nabla \\log P(a_t|s_t) R_t\n",
    "$$\n",
    "\n",
    "policy gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@curiousily/solving-an-mdp-with-q-learning-from-scratch-deep-reinforcement-learning-for-hackers-part-1-45d1d360c120\n",
    "\n",
    "https://github.com/aamini/introtodeeplearning/blob/master/lab3/RL.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "real life: run until termination!!!!! Requires simulator!!!!\n",
    "- realistic simulation + transfer learning\n",
    "- real world observations + one shot trial & error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://arxiv.org/abs/1811.12560\n",
    "- https://pathmind.com/wiki/deep-reinforcement-learning    \n",
    "- https://towardsdatascience.com/why-going-from-implementing-q-learning-to-deep-q-learning-can-be-difficult-36e7ea1648af   \n",
    "https://towardsdatascience.com/policy-based-reinforcement-learning-the-easy-way-8de9a3356083"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/deepmind/pysc2\n",
    "- http://chris-chris.ai/2017/08/30/pysc2-tutorial1/\n",
    "- https://github.com/chris-chris/pysc2-examples\n",
    "- https://blog.goodaudience.com/lessons-and-mistakes-from-my-first-reinforcement-learning-starcraft-agent-4245cc35e956\n",
    "- https://faculty.utrgv.edu/dongchul.kim/csci4352/spring2019/report/R5.pdf\n",
    "- http://courses.cecs.anu.edu.au/courses/CSPROJECTS/19S1/reports/u6049249_report.pdf\n",
    "- https://chatbotslife.com/building-a-smart-pysc2-agent-cdc269cb095d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
