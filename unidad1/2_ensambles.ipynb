{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árboles de decisión y Métodos de ensamble\n",
    "\n",
    "Slides [aquí](https://docs.google.com/presentation/d/1pxJk4cpI_gpvLhDi86EISHjggdyD95K6PgwKlJplkTg/edit?usp=sharing)\n",
    "\n",
    "Material adicional: Capítulos 10 (boosting) y 15 (bagging, random forest) de \"Elements of Statistical Learning\" (ver README del repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import animation\n",
    "from functools import partial\n",
    "slider_layout = widgets.Layout(width='600px', height='20px')\n",
    "slider_style = {'description_width': 'initial'}\n",
    "IntSlider_nice = partial(widgets.IntSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "FloatSlider_nice = partial(widgets.FloatSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "SelSlider_nice = partial(widgets.SelectionSlider, style=slider_style, layout=slider_layout, continuous_update=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs, make_moons\n",
    "\n",
    "N = 1000  \n",
    "X, Y = make_moons(n_samples=N, noise=0.3)\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.scatter(X[Y==0, 0], X[Y==0, 1], c='r', marker='o', \n",
    "           s=10, alpha=0.5, label='class 1')\n",
    "ax.scatter(X[Y==1, 0], X[Y==1, 1], c='b', marker='x', \n",
    "           s=10, alpha=0.5, label='class 2')\n",
    "plt.legend()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.75, test_size=0.25)\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Árbol de decisión con Scikit-Learn\n",
    "\n",
    "- Secuencia de operadores relacionales sobre los atributos en forma de árbol\n",
    "- Los nodos \"hoja\" están asociados a una etiqueta (clasificación)\n",
    "- Los nodos intermedios separan los datos (split)\n",
    "- Las separaciones se seleccionan usando la ganancia de información (*entropy*) o el índice de gini \n",
    "\n",
    "\n",
    "Visualizemos un árbol entrenado para separar este conjunto de datos usando distintas profundidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4), tight_layout=True)\n",
    "\n",
    "def update_plot(md):\n",
    "    model = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=md)\n",
    "    model.fit(X_train, Y_train)\n",
    "    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    for ax_ in ax:\n",
    "        ax_.cla()\n",
    "        ax_.contourf(xx, yy, Z.reshape(xx.shape), cmap=plt.cm.RdBu, alpha=0.5)\n",
    "    ax[0].set_title('Entrenamiento'); ax[1].set_title('Validación')\n",
    "    ax[0].scatter(X_train[Y_train==0, 0], X_train[Y_train==0, 1], color='k', s=10, marker='o', alpha=0.5)\n",
    "    ax[0].scatter(X_train[Y_train==1, 0], X_train[Y_train==1, 1], color='k', s=10, marker='x', alpha=0.5)\n",
    "    ax[1].scatter(X_test[Y_test==0, 0], X_test[Y_test==0, 1], color='k', s=10, marker='o', alpha=0.5)\n",
    "    ax[1].scatter(X_test[Y_test==1, 0], X_test[Y_test==1, 1], color='k', s=10, marker='x', alpha=0.5)\n",
    "\n",
    "\n",
    "widgets.interact(update_plot, md=IntSlider_nice(min=1, max=51));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizemos su desempeño en entrenamiento y validación usando curvas ROC\n",
    "\n",
    "¿Con que profundidad comienza el sobreajuste?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4), tight_layout=True, sharex=True, sharey=True)\n",
    "ax[0].set_title('Entrenamiento')\n",
    "ax[1].set_title('Validación')\n",
    "ax[0].set_xlabel('FPR')\n",
    "ax[0].set_ylabel('TPR')\n",
    "ax[0].set_ylim([0.0, 1.0])\n",
    "\n",
    "\n",
    "for max_depth in [1, 2, 4, 6, 8, 10, 15, 20]:\n",
    "    model = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', \n",
    "                                        max_depth=max_depth)\n",
    "    model.fit(X_train, Y_train)\n",
    "    fpr, tpr, th = roc_curve(Y_train, model.predict_proba(X_train)[:, 1])\n",
    "    ax[0].plot(fpr, tpr, label=str(max_depth), linewidth=1)\n",
    "    fpr, tpr, th = roc_curve(Y_test, model.predict_proba(X_test)[:, 1])\n",
    "    ax[1].plot(fpr, tpr, label=str(max_depth), linewidth=1)\n",
    "    print(max_depth, \" \", auc(fpr, tpr))\n",
    "plt.legend(loc=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar [`sklearn.model_selection.GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) para hacer búsqueda automática de parámetros \n",
    "\n",
    "- Seleccionamos los valores de los parámetros que queremos probar con un diccionario\n",
    "- Seleccionamos un modelo de *scikit-learn*\n",
    "- Seleccionamos el número de *folds*: `cv`\n",
    "- El mejor predictor se recupera con el atributo: `best_estimator_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'criterion':('entropy', 'gini'), 'max_depth':[1, 2, 4, 5, 6, 8, 10, 20, 50]}\n",
    "model = tree.DecisionTreeClassifier(splitter='best')\n",
    "clf = GridSearchCV(model, params, cv=5)\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = clf.best_estimator_\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4), tight_layout=True)\n",
    "ax[0].contourf(xx, yy, Z.reshape(xx.shape), cmap=plt.cm.RdBu, alpha=0.5)\n",
    "ax[0].scatter(X[Y==0, 0], X[Y==0, 1], color='k', s=10, marker='o', alpha=0.5)\n",
    "ax[0].scatter(X[Y==1, 0], X[Y==1, 1], color='k', s=10, marker='x', alpha=0.5)\n",
    "fpr, tpr, th = roc_curve(Y_train, model.predict_proba(X_train)[:, 1])\n",
    "ax[1].plot(fpr, tpr, label='Entrenamiento', linewidth=2)\n",
    "fpr, tpr, th = roc_curve(Y_test, model.predict_proba(X_test)[:, 1])\n",
    "ax[1].plot(fpr, tpr, label='Validación', linewidth=2)\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "ax[1].set_ylim([0.0, 1.0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Random Forest con Scikit-Learn\n",
    "\n",
    "\n",
    "- Conjunto de árboles de decisión entrenados en paralelo usando bootstrap (similar a bagging)\n",
    "- Cada árbol se entrena con un subconjunto de los atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "ensemble.RandomForestClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ensemble.RandomForestClassifier(max_depth=10, n_estimators=100, criterion='entropy',\n",
    "                               n_jobs=2, max_features=2)\n",
    "\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4), tight_layout=True)\n",
    "ax[0].contourf(xx, yy, Z.reshape(xx.shape), cmap=plt.cm.RdBu, alpha=0.5)\n",
    "ax[0].scatter(X[Y==0, 0], X[Y==0, 1], color='k', s=10, marker='o', alpha=0.5)\n",
    "ax[0].scatter(X[Y==1, 0], X[Y==1, 1], color='k', s=10, marker='x', alpha=0.5)\n",
    "fpr, tpr, th = roc_curve(Y_train, model.predict_proba(X_train)[:, 1])\n",
    "ax[1].plot(fpr, tpr, label='Entrenamiento', linewidth=2)\n",
    "fpr, tpr, th = roc_curve(Y_test, model.predict_proba(X_test)[:, 1])\n",
    "ax[1].plot(fpr, tpr, label='Validación', linewidth=2)\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "ax[1].set_ylim([0.0, 1.0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Gradient Boosting con Scikit-Learn\n",
    "\n",
    "- Generaliza el concepto de boosting a cualquier función de costo derivable\n",
    "- Cada clasificador en la cadena se entrena con los residuos del clasificador anterior\n",
    "- Esta implementación usa árboles como clasificador débil\n",
    "Parámetros ajustables:\n",
    "- n_estimators: Número de árboles\n",
    "- max_depth: Profundidad de los árboles\n",
    "- learning_rate: Se usa para disminuir la contribución de cada árbol\n",
    "- max_features: Número de atributos a considerar en cada split (reduce la varianza)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.GradientBoostingClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ensemble.GradientBoostingClassifier(loss='deviance', max_depth=1, max_features=2, \n",
    "                                   n_estimators=1, learning_rate=0.1)\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "print(\"Classification score:\")\n",
    "print(\"Train: %f\" % (model.score(X_train, Y_train)))\n",
    "print(\"Test: %f\" % (model.score(X_test, Y_test)))\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.contourf(xx, yy, Z.reshape(xx.shape), cmap=plt.cm.RdBu, alpha=0.5)\n",
    "ax.scatter(X[Y==0, 0], X[Y==0, 1], color='k', marker='o', alpha=0.5)\n",
    "ax.scatter(X[Y==1, 0], X[Y==1, 1], color='k', marker='x', alpha=0.5)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "fpr, tpr, th = roc_curve(Y_train, model.predict_proba(X_train)[:, 1])\n",
    "ax.plot(fpr, tpr, label='Train', linewidth=4)\n",
    "fpr, tpr, th = roc_curve(Y_test, model.predict_proba(X_test)[:, 1])\n",
    "ax.plot(fpr, tpr, label='Test', linewidth=4)\n",
    "plt.grid()\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "ax.set_ylim([0.0, 1.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "params = {'criterion':('entropy', 'gini'), 'max_depth':[1, 2, 4, 5, 6, 8, 10, 20, 50]}\n",
    "model = tree.ensemble(splitter='best')\n",
    "clf = GridSearchCV(model, params, cv=5)\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [UCI Spam database](https://archive.ics.uci.edu/ml/datasets/Spambase)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -c https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
    "!head spambase.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('spambase.data', delimiter=',')\n",
    "X, Y = data[:, :-1], data[:, -1]\n",
    "\n",
    "display(X.shape)\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "ax.hist(Y_test);\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.75, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'criterion':('entropy', 'gini'), 'max_depth':[1, 5, 10, 20, 50]}\n",
    "model = tree.DecisionTreeClassifier()\n",
    "clf_dt = GridSearchCV(model, params, cv=5)\n",
    "clf_dt.fit(X_train, Y_train)\n",
    "display(clf_dt.best_estimator_)\n",
    "\n",
    "params = {'criterion':('entropy', 'gini'), 'n_estimators':[100], 'class_weight': (None, 'balanced'),\n",
    "          'max_depth':[1, 5, 10, 20, 50], 'max_features': ('sqrt', None)}\n",
    "model = ensemble.RandomForestClassifier(n_jobs=-1)\n",
    "clf_rf = GridSearchCV(model, params, cv=5)\n",
    "clf_rf.fit(X_train, Y_train)\n",
    "display(clf_rf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(5, 4), tight_layout=True)\n",
    "ax.set_xlabel('Recall/TPR')\n",
    "ax.set_ylabel('Precision')\n",
    "\n",
    "Y_pred = clf_dt.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "precision, recall, th = precision_recall_curve(Y_test, Y_pred)\n",
    "ax.plot(recall, precision, label=\"Decision Tree\", linewidth=1)\n",
    "\n",
    "Y_pred = clf_rf.predict_proba(X_test)[:, 1]\n",
    "precision, recall, th = precision_recall_curve(Y_test, Y_pred)\n",
    "ax.plot(recall, precision, label=\"Random Forest\", linewidth=1)\n",
    "plt.legend(loc=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
