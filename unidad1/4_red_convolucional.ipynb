{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import animation\n",
    "from functools import partial\n",
    "slider_layout = widgets.Layout(width='600px', height='20px')\n",
    "slider_style = {'description_width': 'initial'}\n",
    "IntSlider_nice = partial(widgets.IntSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "FloatSlider_nice = partial(widgets.FloatSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "SelSlider_nice = partial(widgets.SelectionSlider, style=slider_style, layout=slider_layout, continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento digital de imágenes\n",
    "\n",
    "Una imagen es una colección de pixeles ordenados\n",
    "\n",
    "En estándar RGB cada pixel corresponde a 3 valores enteros de 8 bit (256 niveles). Combinándolos formamos colores (aproximadamente 16.7M)\n",
    "\n",
    "Otra codificación usual para los pixeles consiste en usar un número entre cero y uno para cada canal (color)\n",
    "\n",
    "El estándar RGBA añade un canal que representa la opacidad\n",
    "\n",
    "Las imágenes en escala de grises y sin opacidad se pueden representar usando un canal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread('cameraman.png')\n",
    "img_bw = 0.2989*img[:, :, 0] + 0.587*img[:, :, 1]+ 0.114*img[:, :, 2]\n",
    "\n",
    "display(img_bw.shape)\n",
    "display(img_bw.dtype)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(img_bw, cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestros sistemas digitales una imagen es una arreglo multidimensional y podemos operarlo como tal\n",
    "\n",
    "A que corresponde este segmento del arreglo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subimg = np.copy(img_bw[50:100, 120:180])\n",
    "display(subimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(subimg, cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y este segmento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(6, 3))\n",
    "ax[1].plot(subimg[30, :])\n",
    "ax[0].imshow(subimg[30:31, :], cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolución  y correlación cruzada discreta\n",
    "\n",
    "Una herramienta clásica de procesamiento digital de señales es la **convolución**\n",
    "\n",
    "La operación de convolución entre dos señales unidimensionales discretas es\n",
    "\n",
    "$$\n",
    "(f * g) [n] = \\sum_{m=-\\infty}^\\infty  f[m] g[n-m]\n",
    "$$\n",
    "\n",
    "y la operación de correlación cruzada es\n",
    "\n",
    "$$\n",
    "(f \\star g) [n] = \\sum_{m=-\\infty}^\\infty  f[m] g[m+n]\n",
    "$$\n",
    "\n",
    "> Para ambas operaciones el resultado es una nueva señal que también depende de  $n$\n",
    "\n",
    "\n",
    "\n",
    "Por ejemplo el elemento $0$ de $f\\star g$ se calcula como\n",
    "\n",
    "    f[0] g[0] + f[1] g[1] + f[2] g[2] + ...\n",
    "\n",
    "Luego el elemento $1$ sería\n",
    "\n",
    "    f[0] g[1] + f[1] g[2] + f[2] g[3] + ...\n",
    "    \n",
    "¿Cómo se ve esta operación graficamente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "plt.close('all'); fig, ax = plt.subplots(2, figsize=(7, 4))\n",
    "ax2 = ax[0].twinx()\n",
    "data = subimg[0, :]\n",
    "\n",
    "def filt(k):\n",
    "    kernel = np.zeros(shape=(len(data),))\n",
    "    kernel[k:k+5] = 1\n",
    "    #kernel[k] = 1.; kernel[k+1] = -1.\n",
    "    return kernel\n",
    "\n",
    "true_filt = filt(0)[np.absolute(filt(0)) >0]\n",
    "display(true_filt)\n",
    "conv_s = scipy.signal.correlate(data, true_filt, mode='valid')\n",
    "\n",
    "\n",
    "def update(k): \n",
    "    ax[0].cla(); ax[1].cla(); ax2.cla();\n",
    "    ax[0].plot(data)\n",
    "    ax2.plot(filt(k), c='r')\n",
    "    ax[1].plot(conv_s); \n",
    "    ax[1].scatter(k, conv_s[k], s=100, c='k')\n",
    "    \n",
    "anim = animation.FuncAnimation(fig, update, frames=len(conv_s), interval=200, blit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado de imágenes con convoluciones\n",
    "\n",
    "Se puede extender el concepto de convolución a dos dimensiones\n",
    "$$\n",
    "(I_1 * I_2) [n_1, n_2] = \\sum_{m_1=-\\infty}^\\infty \\sum_{m_2=-\\infty}^\\infty I_1[m_1, m_2] I_2[n_1-m_2, n_2 - m_2]\n",
    "$$\n",
    "\n",
    "donde $n_1$ es el índice de las filas y $n_2$ es el índice de las columnas\n",
    "\n",
    "#### La convolución entre dos imágenes es una nueva imagen\n",
    "\n",
    "La imagen $I_1$ es la entrada\n",
    "\n",
    "La imagen $I_2$ se denomina filtro o kernel de la convolución\n",
    "\n",
    "La imagen resultante es la imagen filtrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtro pasa-bajo\n",
    "\n",
    "Suaviza, elimina los detalles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 3\n",
    "filt = np.zeros(shape=(D, D))\n",
    "filt[1:-1, 1:-1] = 1\n",
    "display(filt)\n",
    "img_res = scipy.signal.correlate2d(subimg, filt/np.sum(filt), mode='valid')\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True)\n",
    "ax[0].imshow(subimg, cmap=plt.cm.Greys_r)\n",
    "ax[1].imshow(filt, cmap=plt.cm.Greys_r)\n",
    "ax[2].imshow(img_res, cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtro pasa-alto\n",
    "\n",
    "Resalta los cambios bruscos, elimina las partes \"planas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = np.array([[1., -1.]]*2)\n",
    "display(filt)\n",
    "img_res = scipy.signal.correlate2d(subimg, filt, mode='valid')\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True)\n",
    "ax[0].imshow(subimg, cmap=plt.cm.Greys_r)\n",
    "ax[1].imshow(filt, cmap=plt.cm.Greys_r)\n",
    "ax[2].imshow(img_res, cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detector de patillas\n",
    "\n",
    "Detecta patillas de fotografos mirando al horizonte?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = np.ones(shape=(11, 11))\n",
    "filt[:9, 2:9] = 0\n",
    "display(filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_res = scipy.signal.correlate2d(subimg, filt-np.mean(filt), mode='valid')\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True)\n",
    "ax[0].imshow(subimg, cmap=plt.cm.Greys_r)\n",
    "maxloc = np.unravel_index(np.argmax(img_res), shape=img_res.shape)\n",
    "ax[0].scatter(maxloc[1]+filt.shape[0]//2, maxloc[0]+filt.shape[1]//2, c='r', s=20)\n",
    "ax[1].imshow(filt, cmap=plt.cm.Greys_r)\n",
    "ax[2].imshow(img_res, cmap=plt.cm.Reds);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Podríamos aprender filtros para detectar objetos específicos\n",
    "\n",
    "> Necesitamos aprender los valores de los \"píxeles\" del kernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visión computacional\n",
    "\n",
    "La [visión computacional](http://szeliski.org/Book/) es un campo de investigación que busca que los computadores sean capaces de \"comprender\" el contenido presente en imágenes digitales y video\n",
    "\n",
    "#### Objetivo: \n",
    "\n",
    "> Automatizar tareas realizas por el sistema visual humano\n",
    "\n",
    "- Clasificación y Reconocimiento: ¿A qué categoría corresponde el patrón en la imagen?\n",
    "- Detección, Localización y Segmentación: ¿Dónde está el patrón en la imagen? \n",
    "- [Estimación de pose](https://modelzoo.co/blog/deep-learning-models-and-code-for-pose-estimation)\n",
    "- [Reconstrucción](https://www.youtube.com/watch?v=gg0F5JjKmhA), [super-resolución](https://www.extremetech.com/extreme/132950-csi-style-super-resolution-image-enlargment-yeeaaaah) y [síntesis](https://tcwang0509.github.io/pix2pixHD/)\n",
    "- ...\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/detection-and-segmentation-through-convnets-47aa42de27ea\"><img src=\"https://miro.medium.com/max/800/1*SNvD04dEFIDwNAqSXLQC_g.jpeg\" width=\"600\"></a>\n",
    "\n",
    "\n",
    "#### Aplicaciones\n",
    "\n",
    "- [Medicina](https://www.rsipvision.com/medical-segmentation/)\n",
    "- [Navegación autónoma](https://www.youtube.com/watch?v=H7Ym3DMSGms)\n",
    "- [Sistemas de control de tráfico](https://www.youtube.com/watch?v=jxhAWuImxS8)\n",
    "- [Realidad aumentada](https://www.youtube.com/watch?v=r9hVypi_6TQ)\n",
    "- [Agricultura y forestal](https://medium.com/@awangenh/mapping-weeds-and-crops-in-precision-agriculture-with-convolutional-neural-networks-138dab87ba00)\n",
    "- [...](https://www.cs.ubc.ca/~lowe/vision.html)\n",
    "\n",
    "#### Herramientas\n",
    "\n",
    "- Procesamiento digital de imágenes\n",
    "- Optimización, Estadística \n",
    "- Machine learning y en particular  **Redes Neuronales Convolucionales** \n",
    "\n",
    "\n",
    "#### Desafios\n",
    "\n",
    "- Algoritmos invariantes a los cambios de Iluminación\n",
    "- Algoritmos invariantes a los cambios de escala y perspectiva (deformación)\n",
    "- Algoritmos robustos contra la oclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales Convolucionales\n",
    "\n",
    "[Slides 52-92](https://docs.google.com/presentation/d/1IJ2n8X4w8pvzNLmpJB-ms6-GDHWthfsJTFuyUqHfXg8/edit#slide=id.g3a1a71fe7e_8_192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Neuronal Convolucional en PyTorch\n",
    "\n",
    "Las redes neuronales convolucionales utilizan principalmente tres tipos de capas\n",
    "\n",
    "#### [Capas convolucionales](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n",
    "\n",
    "- Las neuronas de estas capas se organizan en filtros \n",
    "- Se realiza la correlación cruzada entre la imagen de entrada y los filtros\n",
    "- Existen capas convolucionales 1D, 2D y 3D\n",
    "- [Visualización de convoluciones con distintos tamaños, strides, padding, dilation](https://github.com/vdumoulin/conv_arithmetic)\n",
    "    \n",
    "    \n",
    "        torch.nn.Conv2d(in_channels, **Cantidad de canales de la imagen de entrada**\n",
    "                        out_channels, **Cantidad de bancos de filtro**\n",
    "                        kernel_size, **Tamaño de los filtros (entero o tupla)**\n",
    "                        stride=1, **Paso de los filtros**\n",
    "                        padding=0, **Cantidad de filas y columnas para agregar a los filtros **\n",
    "                        dilation=1, **Espacio entre los pixeles de los filtros**\n",
    "                        groups=1, **Configuración cruzada entre filtros de entrada y salida**\n",
    "                        bias=True,  **Utilizar sesgo (b)**\n",
    "                        padding_mode='zeros' **Especifica como agregar nuevas filas/columnas (ver padding)**\n",
    "                        )\n",
    "                      \n",
    "      \n",
    "#### [Capas de pooling](https://pytorch.org/docs/stable/nn.html#pooling-layers)\n",
    "\n",
    "- Capa para reducir la dimensión (tamaño) de la capa anterior\n",
    "- Esto reduce la complejidad del modelo y ayuda\n",
    "- Realiza una operación no entrenable: \n",
    "    - Máximo de los pixeles en una región (kernel_size=2, stride=2)\n",
    "    \n",
    "        \n",
    "        1 2 1 0\n",
    "        2 3 1 2      3 2\n",
    "        0 1 0 1      2 1\n",
    "        2 0 0 0\n",
    "        \n",
    "    - Promedio de los píxeles en una región (kernel_size=2, stride=2)\n",
    "    \n",
    "    \n",
    "        1 2 1 0\n",
    "        2 3 1 2      2.00 1.00\n",
    "        0 1 0 1      0.75 0.25\n",
    "        2 0 0 0\n",
    "\n",
    "    torch.nn.MaxPool2d(kernel_size, **Mismo significado que en Conv2d**\n",
    "                       stride=None, **Mismo significado que en Conv2d** \n",
    "                       padding=0, **Mismo significado que en Conv2d**\n",
    "                       dilation=1, **Mismo significado que en Conv2d**\n",
    "                       return_indices=False, **Solo necesario para hacer unpooling**\n",
    "                       ceil_mode=False **Usar ceil en lugar de floor para calcular el tamaño de la salida**\n",
    "                       )\n",
    "        \n",
    "#### [Capas completamente conectadas](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear)\n",
    "\n",
    "- Idénticas a las usadas en redes tipo MLP\n",
    "- Realizan la operación: $Z = WX + b$\n",
    "\n",
    "        torch.nn.Linear(in_features, **Neuronas en la entrada\n",
    "                        out_features,  **Neuronas en la salida**\n",
    "                        bias=True  **Utilizar sesgo (b)**\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mi primera red convolucional para clasificar en pytorch\n",
    "\n",
    "Clasificaremos la base de datos MNIST (10 clases)\n",
    "\n",
    "La arquitectura inicial considera\n",
    "- Una capa convolucional con 8 bancos de filtros\n",
    "- La capa convolucional espera un minibatch de imágenes de 1 canal (blanco y negro)\n",
    "- La capa convolucional usa filtros de 3x3 píxeles\n",
    "- Función de activación [Rectified Linear Unit (ReLU)](https://pytorch.org/docs/stable/nn.html#relu)\n",
    "- Una capa de max-pooling de tamaño 2x2 y stride 2\n",
    "- **Importante** La función `reshape` convierte el tensor de 4 dimensiones a uno de 2 dimensiones. La capa comple\n",
    "- Una capa completamente conectada con 10 neuronas de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class mi_red_convolucional(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(mi_red_convolucional, self).__init__()\n",
    "        # Extracción de características\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3)\n",
    "        self.mpool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Clasificación\n",
    "        self.fc1 = torch.nn.Linear(in_features=8*13*13, out_features=10)\n",
    "\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.mpool1(self.activation(self.conv1(x)))\n",
    "        z = z.reshape(-1, 8*13*13)\n",
    "        return self.fc1(z)\n",
    "    \n",
    "model = mi_red_convolucional()\n",
    "\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones de activación en PyTorch\n",
    "\n",
    "En la red neuronal que acabamos de implementar abandonamos la función de activación sigmoide por la [Rectified Linear Unit (ReLU)](https://pytorch.org/docs/stable/nn.html#relu)\n",
    "\n",
    "Problema de las funciones de activación clásicas (sigmoide y tangente hiperbólica\n",
    "> Extensas zonas saturadas (planas): Cero gradiente\n",
    "\n",
    "Con esto en mente se diseño la función \n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "Que solo satura en los números negativos\n",
    "\n",
    "> En la práctica usar ReLU acelera considerablemente el entrenamiento: más fácil de calcular y menos \"muertes\" de gradiente\n",
    "\n",
    "**Atención**\n",
    "\n",
    "Esta es una área de investigación activa\n",
    "\n",
    "Existen muchas funciones de activación que se han propuesto posterior a ReLU\n",
    "\n",
    "Recomendación: Partir con ReLU y luego probar [otras](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) (LeakyReLU, ELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-5, 5)\n",
    "fig, ax = plt.subplots(2, 2, figsize=(6, 4), tight_layout=True)\n",
    "\n",
    "activation = torch.nn.Sigmoid()\n",
    "y = activation(x)\n",
    "ax[0, 0].plot(x.numpy(), y.numpy());\n",
    "ax[0, 0].set_title('Sigmoide')\n",
    "\n",
    "activation = torch.nn.Tanh()\n",
    "y = activation(x)\n",
    "ax[1, 0].plot(x.numpy(), y.numpy());\n",
    "ax[1, 0].set_title('Tangente Hiperbólica')\n",
    "\n",
    "activation = torch.nn.ReLU()\n",
    "y = activation(x)\n",
    "ax[0, 1].plot(x.numpy(), y.numpy());\n",
    "ax[0, 1].set_title('ReLU')\n",
    "\n",
    "activation = torch.nn.ELU()\n",
    "y = activation(x)\n",
    "ax[1, 1].plot(x.numpy(), y.numpy());\n",
    "ax[1, 1].set_title('ELU');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación multiclase en PyTorch\n",
    "\n",
    "Para hacer clasificación con **más de dos categorías** usamos la [entropía cruzada](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss)\n",
    "\n",
    "    torch.nn.CrossEntropyLoss()\n",
    "\n",
    "Notemos que esto es distinto a la [entropía cruzada binaria](https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss) que usamos para clasificar dos clases\n",
    "\n",
    "Si el problema de clasificación es de $M$ categorías la última capa de la red debe tener $M$ neuronas\n",
    "\n",
    "Adicionalmente no se debe usar función de activación ya que `CrossEntropyLoss` la aplica de forma interna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente descendente con paso adaptivo\n",
    "\n",
    "Para acelerar el entrenamiento podemos usar un algoritmo de [gradiente descendente con paso adaptivo](https://arxiv.org/abs/1609.04747)\n",
    "\n",
    "Un ejemplo ampliamente usado es [Adam](https://arxiv.org/abs/1412.6980)\n",
    "\n",
    "- Se utiliza la historia de los gradientes\n",
    "- Se utiliza momentum (inercia)\n",
    "- Cada parámetro tiene un paso distinto\n",
    "\n",
    "    torch.optim.Adam(params,  **Parámetros de la red neuronal**\n",
    "                     lr=0.001,  **Tasa de aprendizaje inicial**\n",
    "                     betas=(0.9, 0.999),  **Factores de olvido de los gradientes históricos**\n",
    "                     eps=1e-08, **Término para evitar división por cero**\n",
    "                     weight_decay=0, **Regulariza los pesos de la red si es mayor que cero**\n",
    "                     amsgrad=False **Corrección para mejorar la convergencia de Adam en ciertos casos**\n",
    "                     )\n",
    "\n",
    "**Atención**\n",
    "\n",
    "Esta es un área de investigación activa\n",
    "\n",
    "Papers recientes indican que Adam llega a un óptimo más rápido que SGD, pero ese óptimo podría no ser mejor que el obtenido por SGD\n",
    "\n",
    "> Siempre prueba tus redes con distintos optimizadores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, amsgrad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Torchvision](https://pytorch.org/docs/stable/torchvision/index.html)\n",
    "\n",
    "Es una librería utilitaria de PyTorch que facilita considerablemente el trabajo con imágenes\n",
    "\n",
    "- Funcionalidad para descargar sets de benchmark: MNIST, CIFAR, IMAGENET, ...\n",
    "- Modelos clásicos pre-entrenados: Lenet5, AlexNet\n",
    "- Funciones para importar imágenes en distintos formatos\n",
    "- Funciones de transformación para hacer aumentación de datos en imágenes\n",
    "\n",
    "#### Instalación\n",
    "\n",
    "Usando conda o \n",
    "\n",
    "    pip3 install torchvision \n",
    "\n",
    "#### Ejemplo: Obtener base de datos de imágenes de dígitos manuscritos MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "mnist_train_data = torchvision.datasets.MNIST(root='/home/phuijse/datasets/',\n",
    "                                              train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "mnist_test_data = torchvision.datasets.MNIST(root='/home/phuijse/datasets/',\n",
    "                                              train=False, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imágenes de 28x28 píxeles en escala de grises\n",
    "- Diez categorías: Dígitos manuscritos del cero al nueve\n",
    "- 60.000 imágenes de entrenamiento, 10.000 imágenes de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = mnist_train_data[0]\n",
    "display(len(mnist_train_data), type(image), type(label))\n",
    "fig, ax = plt.subplots(1, 10, figsize=(8, 1.5), tight_layout=True)\n",
    "idx = np.random.permutation(len(mnist_train_data))[:10]\n",
    "for k in range(10):\n",
    "    image, label = mnist_train_data[idx[k]]\n",
    "    ax[k].imshow(image[0, :, :].numpy(), cmap=plt.cm.Greys_r)\n",
    "    ax[k].axis('off');\n",
    "    ax[k].set_title(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "import sklearn.model_selection\n",
    "sss = sklearn.model_selection.StratifiedShuffleSplit(train_size=0.75).split(mnist_train_data.data, mnist_train_data.targets)\n",
    "train_idx, valid_idx = next(sss)\n",
    "\n",
    "# Data loader de entrenamiento\n",
    "#train_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "train_dataset = Subset(mnist_train_data, train_idx)\n",
    "#train_data.transforms = train_transforms\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
    "\n",
    "# Data loader de validación\n",
    "#valid_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "valid_dataset = Subset(mnist_train_data, valid_idx)\n",
    "#train_data.transforms = train_transforms\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    nnet = nnet.cuda()\n",
    "\n",
    "# Hiddenlayer objects to track metrics\n",
    "import hiddenlayer as hl\n",
    "history1 = hl.History()\n",
    "canvas1 = hl.Canvas()\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "## tensorboard --logdir=/tmp/tensorboard\n",
    "#writer = SummaryWriter(\"/tmp/tensorboard/net1\")\n",
    "\n",
    "nepochs = 5\n",
    "epoch_loss = np.zeros(nepochs, 2)\n",
    "epoch_acc = np.zeros(nepochs, 2)\n",
    "for epoch in range(nepochs): \n",
    "    epoch_loss, epoch_acc = 0.0, 0.0\n",
    "    # Train\n",
    "    for mbdata, mblabel in train_loader:\n",
    "        if use_gpu:\n",
    "            mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "        prediction = model.forward(mbdata)\n",
    "        optimizer.zero_grad()        \n",
    "        loss = criterion(prediction, mblabel)  \n",
    "        epoch_loss[k, 0] += loss.item()\n",
    "        epoch_acc[k, 0] += (torch.nn.Softmax(dim=1)(prediction).argmax(dim=1) == mblabel).sum().item()        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_loss[k, 0] = epoch_loss[k, 0]/len(train_idx)\n",
    "    epoch_acc[k, 0] = epoch_acc[k, 0]/len(train_idx)\n",
    "    # Validation\n",
    "    #writer.add_scalar('Train/Loss', epoch_loss/len(train_idx), epoch)\n",
    "    #writer.add_scalar('Train/Acc', epoch_acc/len(train_idx), epoch)\n",
    "    epoch_loss, epoch_acc = 0.0, 0.0\n",
    "    for mbdata, mblabel in valid_loader:\n",
    "        if use_gpu:\n",
    "            mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "        prediction = model.forward(mbdata)\n",
    "        loss = criterion(prediction, mblabel)  \n",
    "        epoch_loss[k, 1] += loss.item()\n",
    "        epoch_acc[k, 1] += (torch.nn.Softmax(dim=1)(prediction).argmax(dim=1) == mblabel).sum().item()        \n",
    "    #writer.add_scalar('Valid/Loss', epoch_loss/len(valid_idx), epoch)\n",
    "    #writer.add_scalar('Valid/Acc', epoch_acc/len(valid_idx), epoch)\n",
    "    epoch_loss[k, 1] = epoch_loss[k, 0]/len(valid_idx)\n",
    "    epoch_acc[k, 1] = epoch_acc[k, 0]/len(valid_idx)\n",
    "    history1.log(epoch, loss=epoch_loss[k, 1], accuracy=epoch_acc[k, 1])\n",
    "    with canvas1: # So that they render together\n",
    "        canvas1.draw_plot([history1[\"loss\"]])\n",
    "        canvas1.draw_plot([history1[\"accuracy\"]])\n",
    "    #time.sleep(0.1)\n",
    "\n",
    "if use_gpu:\n",
    "    nnet = model.cpu()\n",
    "    \n",
    "#writer.add_graph(nnet)\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando los filtros aprendidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 8, figsize=(7, 2))\n",
    "w = model.conv1.weight.data.numpy()\n",
    "\n",
    "for i in range(8):    \n",
    "    ax[i].imshow(w[i, 0, :, :])\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando la red neuronal para clasificar ejemplos de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = mnist_test_data[10]\n",
    "y = torch.nn.Softmax(dim=1)(model.forward(image.unsqueeze(0)))\n",
    "display(y)\n",
    "display(torch.argmax(y))\n",
    "display(label)\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(image.numpy()[0, :, :], cmap=plt.cm.Greys_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = []\n",
    "for i in range(len(mnist_test_data)):\n",
    "    image_test, label_test = mnist_test_data[i]\n",
    "    sl = model.forward(image_test.unsqueeze(0))\n",
    "    y = torch.nn.Softmax(dim=1)(sl)\n",
    "    entropy.append(-(y.exp()*y).sum().detach().numpy())  \n",
    "    \n",
    "d = np.argmax(np.array(entropy))\n",
    "print(d, entropy[d])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La predicción para el ejemplo más incierto:\n",
    "image_test, label_test = mnist_test_data[d]\n",
    "sl = model.forward(image_test.unsqueeze(0))\n",
    "y = torch.nn.Softmax(dim=1)(sl)\n",
    "display(y.argmax())\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(image_test[0, :, :].numpy(), cmap=plt.cm.Greys_r);\n",
    "plt.title(label_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aumentación de datos\n",
    "\n",
    "\n",
    "Clase de Lunes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transferencia de Aprendizaje\n",
    "\n",
    "Ajuste fino de un modelo pre-entrenado\n",
    "\n",
    "Clase de Lunes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localización y segmentación\n",
    "\n",
    "Encontrar y segmentar objetos en imágenes\n",
    "\n",
    "Clase de Lunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
